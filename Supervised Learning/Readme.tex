\documentclass{article}
\usepackage{blindtext}
\usepackage{geometry}
\usepackage{hyperref}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\begin{document}
\title{\textbf{Classification Models for Bank Marketing Dataset}}
\author{Rishika Tibrewal, 
Shreyansh Rastogi}
\maketitle
\section{Overview}
The \textbf{Bank Marketing Dataset} from the UCI Machine Learning Repository is related with direct marketing campaigns (phone calls) of a Portuguese banking institution.
The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.
\\
\\
\underline{\textbf{Some of the variables involved:}}
\begin{itemize}
\item age (numeric) 
\item job : type of job (categorical)
\item marital : marital status (categorical) 
\item education (categorical)
\item default: has credit in default? (categorical) 
\item housing: has housing loan? (categorical) 
\item loan: has personal loan? (categorical) 
\item contact: contact communication type (categorical) 
\item month: last contact month of year (categorical) 
\item day\_of\_week: last contact day of the week (categorical)
\item duration: last contact duration, in seconds (numeric)
\item campaign: number of contacts performed during this campaign and for this client (numeric) 
\item pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
\\
\end{itemize}
\href{https://archive.ics.uci.edu/ml/datasets/Bank+Marketing}{\textbf{Click here}} to view the full details of the variables in the source page.
\maketitle
\section{Objective}
The goal is to predict if the client will subscribe a term deposit (variable \textbf{y}) using  classification models.
\maketitle
\section{Methodology}
We have built three classifiers for this dataset: \textbf{Decision tree}, \textbf{Naïve Bayes classifie}r, and \textbf{Random Forest}, and hence compared their performances on this dataset using a suitable metric (here, \textbf{Recall} for \textbf{yes}). 
Cautious Classifier here classifies the interested customers uninterested, which in turn brings down the bank's business, so we have to decrease FN and hence increase recall. Increasing recall makes sense as we have to actually increase the percentage of true positive cases, among all the actual positive ones.
\begin{equation}
Recall = \frac{TP}{TP+FN}\
\end{equation}
\\
The libraries used are \textbf{Pandas, NumPy, Matplotlib, Sklearn, Imblearn, time, seaborn, warnings} and \textbf{memory\_profiler}. 
\maketitle
\section{Data Preprocessing}
\begin{itemize}
\item Starting with Exploratory Data Analysis on the dataset, we try to visualise the categorical columns in all the three models. In \textbf{Naïve Bayes}, we plot the numerical columns as well.
\item We dropped columns \textbf{duration} (as the duration is not known before a call is performed), and \textbf{default} (high proportion of the data here is unknown with a very negligible proportion of yes). In \textbf{Naïve Bayes}, we drop the rows with entry as \textbf{unknown} (as unknown occurs in many columns, and that will in turn increase its occurence probability).
\item We used Label encoding for ordinal columns and One Hot encoding for nominal columns in \textbf{Decision tree} and \textbf{Random Forest}. In \textbf{Naïve Bayes}, we standardised the numerical columns and performed ordinal encoding on categorical columns (we used Pipeline structure for this). 
\item We set aside 30\% of the dataset as the test data and separated the set of attributes and target variable \textbf{y} both in training data and test data.
\item In \textbf{Naïve Bayes:}, we replaced all the outliers with some appropriate values. The value '999' in \textbf{pdays} (meaning that the client was not previously contacted) was replaced by 0, and the values in \textbf{campaign} which are greater than 20 are replaced by the mean value. 
\end{itemize}
\maketitle
\section{Implementation}
\begin{itemize}
\item \textbf{Decision Tree:}
Hyperparameter tuning is performed to find the optimum max\_depth and min\_samples\_split in the tree. Further since, data is highly imbalanced (\textbf{no} is a majority here, and \textbf{yes} is a minority), 
we compute and assign weights to the classes, giving more weight to \textbf{yes}. The score of this model is about 82.83\%. 
\item \textbf{Random Forest:} The Random Forest is applied on the sampled rows (using random over sampling with sampling strategy of 0.5) of training dataset. The sampling strategy would ensure that the minority class was oversampled to have half the number of examples as the majority class, for this binary classification problem (as the data is imbalanced). The score of this model is about 86.63\%.
\item \textbf{Naïve Bayes:} We fitted Gaussian Naïve Bayes on numerical columns and Categorical Naïve Bayes on categorical columns. Next we computed the probability of obtaining this data from each of these Naïve Bayes models and merged them. We fitted the Gaussian Naïve Bayes on these probabilities. The score of this model is about 87.07\%.
\end{itemize}
\maketitle
\section{Comparison of Measures}
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
 \hline
 \textbf{Performance Measure} & \textbf{Decision Tree} & \textbf{Naïve Bayes} & \textbf{Random Forest} \\
 \hline
 Precision & 36\% & 43\% & 43\%\\
 \hline
 Accuracy & 83\% & 87\% & 87\% \\
 \hline
Recall & 64\% & 46\% & 58\% \\
\hline
 Time(ms) & 346 & 306 & 1260 \\
\hline
 Space(MiB) & 348 & 294 & 316 \\
 \hline
\end{tabular}
\end{center}
\href{https://drive.google.com/drive/folders/1OyzkKjy7UCJ0wfkkDQKBc2BCuInMIrXG?usp=sharing}{\textbf{Click here}} to view the output.
\maketitle
\section{Conclusion}
\begin{itemize}
\item Dataset has highly imbalanced data of \textbf{'yes'} \& \textbf{'no'}. Dataset has outliers in the columns \textbf{campaign} \& \textbf{pdays} which we replaced by appropriate values. 
\item For \textbf{Decision tree} and \textbf{Random Forest}, we had to perform precision-recall trade-off. We tried to increase the \textbf{recall} of \textbf{yes} without decreasing accuracy and precision of the model much. 
\item Clearly, in terms of \textbf{accuracy}, both \textbf{Random Forest and Naïve Bayes} are performing better than \textbf{Decision tree}. In terms of \textbf{recall} of \textbf{yes}, we can choose \textbf{Decision tree} to be our preferred model in our case.
\end{itemize}
\end{document}
